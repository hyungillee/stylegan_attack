{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device(\"cuda:0\")\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/pacifinapacific/StyleGAN_LatentEditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "import pickle\n",
    "from urllib.request import urlopen\n",
    "\n",
    "''' 급 오류남\n",
    "import requests\n",
    "r = requests.get('http://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
    "print(r.content)\n",
    "이미지넷(ImageNet)에 정의된 1,000개의 레이블(클래스) 정보 가져오기\n",
    "imagenet_json, _ = urlretrieve('http://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
    "with open(imagenet_json) as f:\n",
    "   imagenet_labels = json.load(f)\n",
    "print(imagenet_labels[18])\n",
    "'''\n",
    "\n",
    "imagenet_labels= pickle.load(urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl')) \n",
    "print(imagenet_labels[18])\n",
    "imagenet_labels = list(imagenet_labels.values())\n",
    "print(imagenet_labels[18] )\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(imsize),\n",
    "    transforms.CenterCrop(imsize), \n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "def image_loader(path):\n",
    "    image = PIL.Image.open(path)\n",
    "    image = preprocess(image).unsqueeze(0) #베치 차원\n",
    "    return image.to(device, torch.float) # raw한 이미지 GPU로 올릴때 float로 올려주기 \n",
    "\n",
    "#image= image_loader('./cat2.jpg')\n",
    "\n",
    "def imshow(tensor):\n",
    "    # matplotlib는 CPU 기반이므로 CPU로 옮기기 기억하기\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0) #배치차원 제거\n",
    "    # PIL 객체로 변경 \n",
    "    image = transforms.ToPILImage()(image) #토치 transform기능중 PIL로 변경\n",
    "    # 이미지를 화면에 출력(matplotlib는 [0, 1] 사이의 값이라고 해도 정상적으로 처리)\n",
    "    plt.imshow(image)\n",
    "# plt.figure()\n",
    "# imshow(image)\n",
    "class Normalize(nn.Module) :\n",
    "    def __init__(self, mean, std) :\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  #Inception v3 의 정규화\n",
    "    torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
    ").to(device).eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "%cd ./StyleGAN_LatentEditor/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stylegan_layers import G_mapping, G_synthesis\n",
    "from collections import OrderedDict\n",
    "\n",
    "g_all = nn.Sequential(OrderedDict([\n",
    "    ('g_mapping', G_mapping()),\n",
    "    ('g_synthesis', G_synthesis(resolution=256))  \n",
    "]))\n",
    "\n",
    "g_all.load_state_dict(torch.load(\"karras2019stylegan-cats-256x256.pt\", map_location=device))\n",
    "g_all.eval()\n",
    "g_all.to(device)\n",
    "\n",
    "g_mapping, generator = g_all[0], g_all[1]\n",
    "\n",
    "#generator\n",
    "noise_grad_offset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stylegan_layers import G_mapping as G_m\n",
    "# from stylegan_layers import G_synthesis as G_s\n",
    "# g_all2 = nn.Sequential(OrderedDict([\n",
    "#     ('g_mapping', G_m()),\n",
    "#     ('g_synthesis', G_s(resolution=256))  \n",
    "# ]))\n",
    "\n",
    "# g_all2.load_state_dict(torch.load(\"karras2019stylegan-cats-256x256.pt\", map_location=device))\n",
    "# g_all2.eval()\n",
    "# g_all2.to(device)\n",
    "\n",
    "# g_mapping2, generator2 = g_all2[0], g_all2[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.use_noise = False\n",
    "class Identity_noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity_noise, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "generator.blocks[\"4x4\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"4x4\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"8x8\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"8x8\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"16x16\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"16x16\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"32x32\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"32x32\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"64x64\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"64x64\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"128x128\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"128x128\"].epi2.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"256x256\"].epi1.top_epi.noise = Identity_noise()\n",
    "generator.blocks[\"256x256\"].epi2.top_epi.noise = Identity_noise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.use_styles = False\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x,latent):\n",
    "        return x\n",
    "    \n",
    "generator.blocks[\"4x4\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"4x4\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"8x8\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"8x8\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"16x16\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"16x16\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"32x32\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"32x32\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"64x64\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"64x64\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"128x128\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"128x128\"].epi2.style_mod = Identity()\n",
    "generator.blocks[\"256x256\"].epi1.style_mod = Identity()\n",
    "generator.blocks[\"256x256\"].epi2.style_mod = Identity()\n",
    "generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 클린 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import torchvision\n",
    "noise_grad_offset = False\n",
    "nb_rows = 3\n",
    "nb_cols = 8\n",
    "nb_samples = nb_rows * nb_cols\n",
    "# g_all.eval()\n",
    "# g_all.to(device)\n",
    "latents = torch.randn(nb_samples, 512, device=device,requires_grad=False)\n",
    "latents=g_mapping(latents)\n",
    "print(latents.shape)\n",
    "synth_img = generator(latents)\n",
    "\n",
    "imgs= ((synth_img + 1.0) / 2.0).detach().cpu()\n",
    "print(imgs.shape)\n",
    "\n",
    "imgs = torchvision.utils.make_grid(imgs, nrow=nb_cols)\n",
    "pyplot.figure(figsize=(15, 6))\n",
    "pyplot.imshow(imgs.permute(1, 2, 0).numpy() )\n",
    "\n",
    "# for i in range(16):\n",
    "#     plt.imshow( ( (synth_img + 1.0) / 2.0).detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_all.eval()\n",
    "g_all.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W벡터 2개씩만 써보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "\n",
    "#latent= torch.randn_like(latent,requires_grad=True,device=device)\n",
    "ran=0\n",
    "for i in range(0,18,2):\n",
    "    synth_img = generator(latent,i)\n",
    "    plt.imshow( ( (synth_img + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "\n",
    "#latent= torch.randn_like(latent,requires_grad=True,device=device)\n",
    "synth_img = generator(latent)\n",
    "\n",
    "plt.imshow( ( (synth_img + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하나씩 꺼보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x,latent):\n",
    "        return x\n",
    "        \n",
    "latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "#latent= torch.randn_like(latent,requires_grad=True,device=device)\n",
    "synth_img = generator(latent)\n",
    "\n",
    "cnt=1\n",
    "st = 4\n",
    "epinum=1\n",
    "for _ in range(14):\n",
    "    g_all = nn.Sequential(OrderedDict([\n",
    "        ('g_mapping', G_mapping()),\n",
    "        ('g_synthesis', G_synthesis(resolution=256))  \n",
    "    ]))\n",
    "    g_all.load_state_dict(torch.load(\"karras2019stylegan-cats-256x256.pt\", map_location=device))\n",
    "    g_all.eval()\n",
    "    g_all.to(device)\n",
    "    g_mapping, generator = g_all[0], g_all[1]\n",
    "    \n",
    "    if cnt%2==0:\n",
    "        epinum=2\n",
    "    else:\n",
    "        epinum=1    \n",
    "    exec(f'generator.blocks[\"{st}x{st}\"].epi{epinum}.style_mod = Identity()')\n",
    "    if cnt%2==0:\n",
    "        st*=2\n",
    "    synth_img = generator(latent)\n",
    "    \n",
    "    plt.imshow( ( (synth_img + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    cnt+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Identity_noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity_noise, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "#generator.blocks[\"4x4\"].epi1.top_epi.noise = Identity_noise()\n",
    "        \n",
    "latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "#latent= torch.randn_like(latent,requires_grad=True,device=device)\n",
    "synth_img = generator(latent)\n",
    "\n",
    "cnt=1\n",
    "st = 4\n",
    "epinum=1\n",
    "for _ in range(14):\n",
    "    g_all = nn.Sequential(OrderedDict([\n",
    "        ('g_mapping', G_mapping()),\n",
    "        ('g_synthesis', G_synthesis(resolution=256))  \n",
    "    ]))\n",
    "    g_all.load_state_dict(torch.load(\"karras2019stylegan-cats-256x256.pt\", map_location=device))\n",
    "    g_all.eval()\n",
    "    g_all.to(device)\n",
    "    g_mapping, generator = g_all[0], g_all[1]\n",
    "    \n",
    "    if cnt%2==0:\n",
    "        epinum=2\n",
    "    else:\n",
    "        epinum=1    \n",
    "    exec(f'generator.blocks[\"{st}x{st}\"].epi{epinum}.top_epi.noise = Identity_noise()')\n",
    "    if cnt%2==0:\n",
    "        st*=2\n",
    "    synth_img = generator(latent)\n",
    "    \n",
    "    plt.imshow( ( (synth_img + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent = torch.zeros((1, 18, 512), requires_grad=False, device=device)\n",
    "\n",
    "latents = torch.randn(8, 512, device=device)\n",
    "latent_zero= torch.zeros((1,512),  device=device)\n",
    "for i in range(10):#\n",
    "    #latents = torch.randn(8, 512, device=device)\n",
    "    synth_img = g_all(latent_zero)\n",
    "    #plt.imshow( synth_img [0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.imshow( ( (synth_img + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def unrestricted_attack(generator, classifier, ori_label, trg_label, eps, alpha, iters,targeted):\n",
    "    trg_label = trg_label.to(device)\n",
    "    attack_success_list=[]\n",
    "    latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "    perturbation = torch.empty_like(latent).uniform_(-eps, eps)\n",
    "    perturbation.to(device)\n",
    "\n",
    "    attack_loss = nn.CrossEntropyLoss() # 타겟티드 어택으로 목표 클래스로 분류되도록\n",
    "    mse = torch.nn.MSELoss(reduction='sum') # 초기 이미지와 유사하도록 , 출력값 다 더함\n",
    "\n",
    "    # 시작 당시의 이미지 정보\n",
    "    start_generated = generator(latent)\n",
    "\n",
    "    print(\"[ 현재 이미지 ]\")\n",
    "    plt.imshow(((start_generated + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(iters):\n",
    "        perturbation.requires_grad = True\n",
    "        now = latent + perturbation\n",
    "\n",
    "        generator.zero_grad()\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        generated = generator(now)\n",
    "        outputs = classifier((generated + 1.0) / 2.0)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f\"[ i = {i} ]\")\n",
    "            print(\"[ 현재 이미지 ]\")\n",
    "            plt.imshow(((generated + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "            plt.show()\n",
    "            percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "            print(\"< 가장 높은 확률을 가지는 클래스들 >\")\n",
    "            for i in outputs[0].topk(3)[1]:\n",
    "                print(f\"인덱스: {i.item()} / 클래스명: {imagenet_labels[i]} / 확률: {round(percentages[i].item(), 4)}%\")\n",
    "\n",
    "        percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "        if outputs[0].topk(1)[1] == 18 and round(percentages[18].item(), 4)>=90. :\n",
    "            attack_success_list.append(f'{i} 번째 :,{round(percentages[18].item(), 4)}% 로 성공' )\n",
    "                \n",
    "        cost = attack_loss(outputs, trg_label).to(device) * 1e3 #1000을곱함\n",
    "        cost += mse(generated, start_generated).to(device) #현재 입력과 만들어진 입력간의 mse \n",
    "        cost.backward(retain_graph=True) # 시작 이미지에 대한 기울기 정보는 유지(retain)될 수 있도록\n",
    "\n",
    "        if targeted:\n",
    "            # 계산된 기울기(gradient)를 이용하여 손실 함수가 감소하는 방향으로 업데이트\n",
    "            diff = - alpha * perturbation.grad.sign()\n",
    "        else:\n",
    "            diff = alpha * perturbation.grad.sign()\n",
    "        perturbation = torch.clamp(perturbation + diff, min=-eps, max=eps).detach_()\n",
    "\n",
    "    return latent, perturbation,attack_success_list#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공격용 파라미터 설정\n",
    "targeted = True\n",
    "\n",
    "eps = 8/255\n",
    "alpha = 0.5/255\n",
    "iters = 1000\n",
    "\n",
    "# 얼룩 고양이(tabby cat)으로 보이도록 설정. \n",
    "ori_label = \"tabby cat\" # index: 281\n",
    "\n",
    "# 까치(magpie)로 분류되도록 공격 수행\n",
    "trg_label = [18]\n",
    "trg_label = torch.Tensor(trg_label)\n",
    "trg_label = trg_label.type(torch.long)\n",
    "\n",
    "latent, perturbation,attack_success_list = unrestricted_attack(generator, classifier, ori_label, trg_label, eps, alpha, iters,targeted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_success_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 논타켓티드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targeted = False\n",
    "eps = 8/255\n",
    "alpha = 2/255\n",
    "iters = 200\n",
    "\n",
    "# 얼룩 고양이(tabby cat)로 분류가 되지 않도록 공격 수행\n",
    "trg_label = [281]\n",
    "trg_label = torch.Tensor(trg_label)\n",
    "trg_label = trg_label.type(torch.long)\n",
    "\n",
    "latent, perturbation,attack_success_list = unrestricted_attack(generator, classifier, ori_label, trg_label, eps, alpha, iters,targeted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노이즈 어택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturshape = torch.zeros((512,4,4), requires_grad=True, device=device)\n",
    "perturb = torch.empty_like(perturshape).uniform_(-eps, eps)\n",
    "perturb_param = torch.nn.Parameter(perturb, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def unrestricted_attack(generator, classifier, ori_label, trg_label, eps, alpha, iters,targeted):\n",
    "    trg_label = trg_label.to(device)\n",
    "    attack_success_list=[]\n",
    "    latent = torch.zeros((1, 18, 512), requires_grad=True, device=device)\n",
    "    \n",
    "    perturshape = torch.zeros((512,4,4), requires_grad=True, device=device)\n",
    "    perturb = torch.empty_like(perturshape).uniform_(-eps, eps)\n",
    "    perturb_param = torch.nn.Parameter(perturb, requires_grad=True)\n",
    "    \n",
    "#     noise_pertur = torch.empty_like(perturshape).uniform_(-eps, eps)\n",
    "    global noise_pertur\n",
    "    noise_pertur = perturb_param\n",
    "    noise_pertur.to(device)\n",
    "    print(\"noise_pertur 초기화\",noise_pertur)\n",
    "\n",
    "    attack_loss = nn.CrossEntropyLoss() # 타겟티드 어택으로 목표 클래스로 분류되도록\n",
    "    mse = torch.nn.MSELoss(reduction='sum') # 초기 이미지와 유사하도록 , 출력값 다 더함\n",
    "\n",
    "    # 시작 당시의 이미지 정보\n",
    "    global noise_grad_offset\n",
    "    noise_grad_offset = False\n",
    "    start_generated = generator(latent)\n",
    "\n",
    "    print(\"[ 현재 이미지 ]\")\n",
    "    plt.imshow(((start_generated + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    \n",
    "    noise_grad_offset = True\n",
    "    for i in range(iters):\n",
    "        noise_pertur.requires_grad = True\n",
    "        \n",
    "        #now = latent + perturbation\n",
    "\n",
    "        generator.zero_grad()\n",
    "        classifier.zero_grad()\n",
    "\n",
    "        generated = generator(latent)\n",
    "        outputs = classifier((generated + 1.0) / 2.0)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f\"[ i = {i} ]\")\n",
    "            print(\"[ 현재 이미지 ]\")\n",
    "            plt.imshow(((generated + 1.0) / 2.0)[0].detach().cpu().squeeze(0).permute(1, 2, 0))\n",
    "            plt.show()\n",
    "            percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "            print(\"< 가장 높은 확률을 가지는 클래스들 >\")\n",
    "            for i in outputs[0].topk(3)[1]:\n",
    "                print(f\"인덱스: {i.item()} / 클래스명: {imagenet_labels[i]} / 확률: {round(percentages[i].item(), 4)}%\")\n",
    "\n",
    "        percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
    "        if outputs[0].topk(1)[1] == 18 and round(percentages[18].item(), 4)>=90. :\n",
    "            attack_success_list.append(f'{i} 번째 :,{round(percentages[18].item(), 4)}% 로 성공' )\n",
    "                \n",
    "        cost = attack_loss(outputs, trg_label).to(device) * 1e3 #1000을곱함\n",
    "        cost += mse(generated, start_generated).to(device) #현재 입력과 만들어진 입력간의 mse \n",
    "        cost.backward(retain_graph=True) # 시작 이미지에 대한 기울기 정보는 유지(retain)될 수 있도록\n",
    "        print(noise_pertur.grad.data)\n",
    "        if targeted:\n",
    "            # 계산된 기울기(gradient)를 이용하여 손실 함수가 감소하는 방향으로 업데이트\n",
    "            diff = - alpha * noise_pertur.grad.sign()\n",
    "        else:\n",
    "            diff = alpha * noise_pertur.grad.sign()\n",
    "        noise_pertur = torch.clamp(noise_pertur + diff, min=-eps, max=eps).detach_()\n",
    "\n",
    "    return latent, noise_pertur,attack_success_list#\n",
    "\n",
    "targeted = True\n",
    "eps = 8/255\n",
    "alpha = 0.5/255\n",
    "iters = 1000\n",
    "# 얼룩 고양이(tabby cat)으로 보이도록 설정. \n",
    "ori_label = \"tabby cat\" # index: 281\n",
    "# 까치(magpie)로 분류되도록 공격 수행\n",
    "trg_label = [18]\n",
    "trg_label = torch.Tensor(trg_label)\n",
    "trg_label = trg_label.type(torch.long)\n",
    "latent, perturbation,attack_success_list = unrestricted_attack(generator, classifier, ori_label, trg_label, eps, alpha, iters,targeted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_pertur.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc():\n",
    "    global ya\n",
    "    ya=2\n",
    "    c()\n",
    "    print(ya)\n",
    "def c():\n",
    "    global ya\n",
    "    ya+=1\n",
    "    print('c안',ya)\n",
    "    \n",
    "abc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "noise_counter=0\n",
    "\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n",
    "        super().__init__()\n",
    "        he_std = gain * input_size**(-0.5) # He init\n",
    "        # Equalized learning rate and custom learning rate multiplier.\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        return F.linear(x, self.weight * self.w_mul, bias)\n",
    "\n",
    "class MyConv2d(nn.Module):\n",
    "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stride=1, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n",
    "                intermediate=None, upscale=False, downscale=False):\n",
    "        super().__init__()\n",
    "        if upscale:\n",
    "            self.upscale = Upscale2d()\n",
    "        else:\n",
    "            self.upscale = None\n",
    "        if downscale:\n",
    "            self.downscale = Downscale2d()\n",
    "        else:\n",
    "            self.downscale = None\n",
    "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n",
    "        self.kernel_size = kernel_size\n",
    "        if use_wscale:\n",
    "            init_std = 1.0 / lrmul\n",
    "            self.w_mul = he_std * lrmul\n",
    "        else:\n",
    "            init_std = he_std / lrmul\n",
    "            self.w_mul = lrmul\n",
    "        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
    "            self.b_mul = lrmul\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.intermediate = intermediate\n",
    "\n",
    "    def forward(self, x):\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias * self.b_mul\n",
    "        \n",
    "        have_convolution = False\n",
    "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
    "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
    "            # this really needs to be cleaned up and go into the conv...\n",
    "            w = self.weight * self.w_mul\n",
    "            w = w.permute(1, 0, 2, 3)\n",
    "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
    "            w = F.pad(w, (1,1,1,1))\n",
    "            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
    "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
    "            have_convolution = True\n",
    "        elif self.upscale is not None:\n",
    "            x = self.upscale(x)\n",
    "        \n",
    "        downscale = self.downscale\n",
    "        intermediate = self.intermediate\n",
    "        if downscale is not None and min(x.shape[2:]) >= 128:\n",
    "            w = self.weight * self.w_mul\n",
    "            w = F.pad(w, (1,1,1,1))\n",
    "            # in contrast to upscale, this is a mean...\n",
    "            w = (w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1])*0.25 # avg_pool?\n",
    "            x = F.conv2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
    "            have_convolution = True\n",
    "            downscale = None\n",
    "        elif downscale is not None:\n",
    "            assert intermediate is None\n",
    "            intermediate = downscale\n",
    "            \n",
    "        if not have_convolution and intermediate is None:\n",
    "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n",
    "        elif not have_convolution:\n",
    "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
    "\n",
    "        if intermediate is not None:\n",
    "            x = intermediate(x)\n",
    "\n",
    "        if bias is not None:\n",
    "            x = x + bias.view(1, -1, 1, 1)\n",
    "        return x\n",
    "\n",
    "class NoiseLayer(nn.Module):\n",
    "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(channels))\n",
    "        self.noise = None\n",
    "        \n",
    "    def forward(self, x, noise=None):\n",
    "        if noise is None and self.noise is None:\n",
    "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
    "        elif noise is None:\n",
    "            # here is a little trick: if you get all the noiselayers and set each\n",
    "            # modules .noise attribute, you can have pre-defined noise.\n",
    "            # Very useful for analysis\n",
    "            noise = self.noise\n",
    "        global noise_counter\n",
    "        global noise_pertur\n",
    "        noise_counter+=1\n",
    "        global noise_grad_offset\n",
    "        if noise_counter ==1 and noise_grad_offset == True:\n",
    "            print(\"noise pertur 연산수행\")\n",
    "            noise_value = self.weight.view(1, -1, 1, 1) * noise + noise_pertur.to(device)\n",
    "        else:\n",
    "            noise_value = self.weight.view(1, -1, 1, 1) * noise\n",
    "        #print(\"nc\",noise_counter) \n",
    "        x = x + noise_value\n",
    "        #print(\"x 크기\",x.shape)\n",
    "        return x\n",
    "\n",
    "class StyleMod(nn.Module):\n",
    "    def __init__(self, latent_size, channels, use_wscale):\n",
    "        super(StyleMod, self).__init__()\n",
    "        self.lin = MyLinear(latent_size,\n",
    "                            channels * 2,\n",
    "                            gain=1.0, use_wscale=use_wscale)\n",
    "        \n",
    "    def forward(self, x, latent):\n",
    "        style = self.lin(latent) # style => [batch_size, n_channels*2]\n",
    "        #print(\"layer거친 후 style 크기: \",style)\n",
    "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
    "        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
    "        #print(\"Scale. bias 계산 전  style 크기: \",style)\n",
    "        x = x * (style[:, 0] + 1.) + style[:, 1]\n",
    "        #print(\"Scale. bias 계산 후  x 크기: \",x.shape)\n",
    "        return x\n",
    "\n",
    "class PixelNormLayer(nn.Module):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class BlurLayer(nn.Module):\n",
    "    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n",
    "        super(BlurLayer, self).__init__()\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "        kernel = kernel[:, None] * kernel[None, :]\n",
    "        kernel = kernel[None, None]\n",
    "        if normalize:\n",
    "            kernel = kernel / kernel.sum()\n",
    "        if flip:\n",
    "            kernel = kernel[:, :, ::-1, ::-1]\n",
    "        self.register_buffer('kernel', kernel)\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # expand kernel channels\n",
    "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
    "        x = F.conv2d(\n",
    "            x,\n",
    "            kernel,\n",
    "            stride=self.stride,\n",
    "            padding=int((self.kernel.size(2)-1)/2),\n",
    "            groups=x.size(1)\n",
    "        )\n",
    "        return x\n",
    "\n",
    "def upscale2d(x, factor=2, gain=1):\n",
    "    assert x.dim() == 4\n",
    "    if gain != 1:\n",
    "        x = x * gain\n",
    "    if factor != 1:\n",
    "        shape = x.shape\n",
    "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
    "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
    "    return x\n",
    "\n",
    "class Upscale2d(nn.Module):\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor, int) and factor >= 1\n",
    "        self.gain = gain\n",
    "        self.factor = factor\n",
    "    def forward(self, x):\n",
    "        return upscale2d(x, factor=self.factor, gain=self.gain)\n",
    "\n",
    "\n",
    "class G_mapping(nn.Sequential):\n",
    "    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        layers = [\n",
    "            ('pixel_norm', PixelNormLayer()),\n",
    "            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense0_act', act),\n",
    "            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense1_act', act),\n",
    "            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense2_act', act),\n",
    "            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense3_act', act),\n",
    "            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense4_act', act),\n",
    "            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense5_act', act),\n",
    "            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense6_act', act),\n",
    "            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
    "            ('dense7_act', act)\n",
    "        ]\n",
    "        super().__init__(OrderedDict(layers))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        # Broadcast\n",
    "        x = x.unsqueeze(1).expand(-1, 18, -1)\n",
    "        return x\n",
    "\n",
    "class Truncation(nn.Module):\n",
    "    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n",
    "        super().__init__()\n",
    "        self.max_layer = max_layer\n",
    "        self.threshold = threshold\n",
    "        self.register_buffer('avg_latent', avg_latent)\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 3\n",
    "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
    "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n",
    "        return torch.where(do_trunc, interp, x)\n",
    "\n",
    "class LayerEpilogue(nn.Module):\n",
    "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
    "    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if use_noise:\n",
    "            layers.append(('noise', NoiseLayer(channels)))\n",
    "        layers.append(('activation', activation_layer))\n",
    "        if use_pixel_norm:\n",
    "            layers.append(('pixel_norm', PixelNorm()))\n",
    "        if use_instance_norm:\n",
    "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
    "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        if use_styles:\n",
    "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
    "        else:\n",
    "            self.style_mod = None\n",
    "    def forward(self, x, dlatents_in_slice=None,):\n",
    "        x = self.top_epi(x)\n",
    "        \n",
    "        if self.style_mod is not None:\n",
    "            x = self.style_mod(x, dlatents_in_slice)\n",
    "        else:\n",
    "            assert dlatents_in_slice is None\n",
    "        return x\n",
    "\n",
    "\n",
    "class InputBlock(nn.Module):\n",
    "    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        super().__init__()\n",
    "        self.const_input_layer = const_input_layer\n",
    "        self.nf = nf\n",
    "        if self.const_input_layer:\n",
    "            # called 'const' in tf\n",
    "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
    "            self.bias = nn.Parameter(torch.ones(nf))\n",
    "        else:\n",
    "            self.dense = MyLinear(dlatent_size, nf*16, gain=gain/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n",
    "#         print(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        \n",
    "    def forward(self, dlatents_in_range):\n",
    "        batch_size = dlatents_in_range.size(0)\n",
    "        if self.const_input_layer:\n",
    "            x = self.const.expand(batch_size, -1, -1, -1)\n",
    "            x = x + self.bias.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GSynthesisBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
    "        # 2**res x 2**res # res = 3..resolution_log2\n",
    "        super().__init__()\n",
    "        if blur_filter:\n",
    "            blur = BlurLayer(blur_filter)\n",
    "        else:\n",
    "            blur = None\n",
    "        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
    "                                 intermediate=blur, upscale=True)\n",
    "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
    "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
    "            \n",
    "    def forward(self, x, dlatents_in_range):\n",
    "        x = self.conv0_up(x)\n",
    "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
    "        x = self.conv1(x)\n",
    "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
    "        return x\n",
    "\n",
    "\n",
    "class G_synthesis(nn.Module):\n",
    "    def __init__(self,\n",
    "        dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n",
    "        num_channels        = 3,            # Number of output color channels.\n",
    "        resolution          = 1024,         # Output resolution.\n",
    "        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "        use_styles          = True,         # Enable style inputs?\n",
    "        const_input_layer   = True,         # First layer is a learned constant?\n",
    "        use_noise           = True,         # Enable noise inputs?\n",
    "        randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
    "        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n",
    "        use_wscale          = True,         # Enable equalized learning rate?\n",
    "        use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n",
    "        use_instance_norm   = True,         # Enable instance normalization?\n",
    "        dtype               = torch.float32,  # Data type to use for activations and outputs.\n",
    "        fused_scale         = 'auto',       # True = fused convolution + scaling, False = separate ops, 'auto' = decide automatically.\n",
    "        blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "        structure           = 'auto',       # 'fixed' = no progressive growing, 'linear' = human-readable, 'recursive' = efficient, 'auto' = select automatically.\n",
    "        is_template_graph   = False,        # True = template graph constructed by the Network class, False = actual evaluation.\n",
    "        force_clean_graph   = False,        # True = construct a clean graph that looks nice in TensorBoard, False = default behavior.\n",
    "        \n",
    "\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        def nf(stage):\n",
    "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "        self.dlatent_size = dlatent_size\n",
    "\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2**resolution_log2 and resolution >= 4\n",
    "        if is_template_graph: force_clean_graph = True\n",
    "        if force_clean_graph: randomize_noise = False\n",
    "        if structure == 'auto': structure = 'linear' if force_clean_graph else 'recursive'\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        num_layers = resolution_log2 * 2 - 2\n",
    "        num_styles = num_layers if use_styles else 1\n",
    "        torgbs = []\n",
    "        blocks = []\n",
    "        for res in range(2, resolution_log2 + 1):\n",
    "            channels = nf(res-1)\n",
    "            name = '{s}x{s}'.format(s=2**res)\n",
    "            if res == 2:\n",
    "                blocks.append((name,\n",
    "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
    "                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "                \n",
    "            else:\n",
    "                blocks.append((name,\n",
    "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
    "            last_channels = channels\n",
    "        self.torgb = MyConv2d(channels,num_channels, 1,gain=1, use_wscale=use_wscale)\n",
    "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
    "        \n",
    "    def forward(self,dlatents_in):#,ran\n",
    "\n",
    "        batch_size = dlatents_in.size(0)\n",
    "        for i, m in enumerate(self.blocks.values()):\n",
    "            if i == 0:\n",
    "                #x = m(dlatents_in[:, ran:ran+2])\n",
    "                x = m(dlatents_in[:, 2*i:2*i+2])\n",
    "            else:\n",
    "                #print(ran)\n",
    "                #x = m(x,dlatents_in[:, ran:ran+2])\n",
    "                #x = m(x,dlatents_in[:, 0:2])\n",
    "                x = m(x,dlatents_in[:, 2*i:2*i+2])\n",
    "                \n",
    "\n",
    "        rgb = self.torgb(x)\n",
    "\n",
    "        return rgb\n",
    "\n",
    "\n",
    "\n",
    "class StddevLayer(nn.Module):\n",
    "    def __init__(self, group_size=4, num_new_features=1):\n",
    "        super().__init__()\n",
    "        self.group_size = 4\n",
    "        self.num_new_features = 1\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        group_size = min(self.group_size, b)\n",
    "        y = x.reshape([group_size, -1, self.num_new_features,\n",
    "                        c // self.num_new_features, h, w])\n",
    "        y = y - y.mean(0, keepdim=True)\n",
    "        y = (y**2).mean(0, keepdim=True)\n",
    "        y = (y + 1e-8)**0.5\n",
    "        y = y.mean([3, 4, 5], keepdim=True).squeeze(3) # don't keep the meaned-out channels\n",
    "        y = y.expand(group_size, -1, -1, h, w).clone().reshape(b, self.num_new_features, h, w)\n",
    "        z = torch.cat([x, y], dim=1)\n",
    "        return z\n",
    "\n",
    "class Downscale2d(nn.Module):\n",
    "    def __init__(self, factor=2, gain=1):\n",
    "        super().__init__()\n",
    "        assert isinstance(factor, int) and factor >= 1\n",
    "        self.factor = factor\n",
    "        self.gain = gain\n",
    "        if factor == 2:\n",
    "            f = [np.sqrt(gain) / factor] * factor\n",
    "            self.blur = BlurLayer(kernel=f, normalize=False, stride=factor)\n",
    "        else:\n",
    "            self.blur = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim()==4\n",
    "        # 2x2, float32 => downscale using _blur2d().\n",
    "        if self.blur is not None and x.dtype == torch.float32:\n",
    "            return self.blur(x)\n",
    "\n",
    "        # Apply gain.\n",
    "        if self.gain != 1:\n",
    "            x = x * self.gain\n",
    "\n",
    "        # No-op => early exit.\n",
    "        if factor == 1:\n",
    "            return x\n",
    "\n",
    "        # Large factor => downscale using tf.nn.avg_pool().\n",
    "        # NOTE: Requires tf_config['graph_options.place_pruned_graph']=True to work.\n",
    "        return F.avg_pool2d(x, self.factor)\n",
    "\n",
    "class DiscriminatorBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, gain, use_wscale, activation_layer):\n",
    "        super().__init__(OrderedDict([\n",
    "            ('conv0', MyConv2d(in_channels, in_channels, 3, gain=gain, use_wscale=use_wscale)), # out channels nf(res-1)\n",
    "            ('act0', activation_layer),\n",
    "            ('blur', BlurLayer()),\n",
    "            ('conv1_down', MyConv2d(in_channels, out_channels, 3, gain=gain, use_wscale=use_wscale, downscale=True)),\n",
    "            ('act1', activation_layer)]))\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.shape)\n",
    "\n",
    "class DiscriminatorTop(nn.Sequential):\n",
    "    def __init__(self, mbstd_group_size, mbstd_num_features, in_channels, intermediate_channels, gain, use_wscale, activation_layer, resolution=4, in_channels2=None, output_features=1, last_gain=1):\n",
    "        layers = []\n",
    "        if mbstd_group_size > 1:\n",
    "            layers.append(('stddev_layer', StddevLayer(mbstd_group_size, mbstd_num_features)))\n",
    "        if in_channels2 is None:\n",
    "            in_channels2 = in_channels\n",
    "        layers.append(('conv', MyConv2d(in_channels + mbstd_num_features, in_channels2, 3, gain=gain, use_wscale=use_wscale)))\n",
    "        layers.append(('act0', activation_layer))\n",
    "        layers.append(('view', View(-1)))\n",
    "        layers.append(('dense0', MyLinear(in_channels2*resolution*resolution, intermediate_channels, gain=gain, use_wscale=use_wscale)))\n",
    "        layers.append(('act1', activation_layer))\n",
    "        layers.append(('dense1', MyLinear(intermediate_channels, output_features, gain=last_gain, use_wscale=use_wscale)))\n",
    "        super().__init__(OrderedDict(layers))\n",
    "\n",
    "class D_basic(nn.Sequential):\n",
    "    \n",
    "    def __init__(self,\n",
    "        #images_in,                          # First input: Images [minibatch, channel, height, width].\n",
    "        #labels_in,                          # Second input: Labels [minibatch, label_size].\n",
    "        num_channels        = 3,            # Number of input color channels. Overridden based on dataset.\n",
    "        resolution          = 1024,           # Input resolution. Overridden based on dataset.\n",
    "        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
    "        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
    "        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
    "        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu',\n",
    "        use_wscale          = True,         # Enable equalized learning rate?\n",
    "        mbstd_group_size    = 4,            # Group size for the minibatch standard deviation layer, 0 = disable.\n",
    "        mbstd_num_features  = 1,            # Number of features for the minibatch standard deviation layer.\n",
    "        #blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
    "                ):\n",
    "        self.mbstd_group_size = 4\n",
    "        self.mbstd_num_features = 1\n",
    "        resolution_log2 = int(np.log2(resolution))\n",
    "        assert resolution == 2**resolution_log2 and resolution >= 4\n",
    "        def nf(stage):\n",
    "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
    "\n",
    "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
    "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
    "        self.gain = gain\n",
    "        self.use_wscale = use_wscale\n",
    "        super().__init__(OrderedDict([\n",
    "            ('fromrgb', MyConv2d(num_channels, nf(resolution_log2-1), 1, gain=gain, use_wscale=use_wscale)),\n",
    "            ('act', act)]\n",
    "            +[('{s}x{s}'.format(s=2**res), DiscriminatorBlock(nf(res-1), nf(res-2), gain=gain, use_wscale=use_wscale, activation_layer=act)) for res in range(resolution_log2, 2, -1)]\n",
    "            +[('4x4', DiscriminatorTop(mbstd_group_size, mbstd_num_features, nf(2), nf(2), gain=gain, use_wscale=use_wscale, activation_layer=act))]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
